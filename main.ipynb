{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_loader(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": os.path.basename(file_path)\n",
    "        }\n",
    "        return [Document(page_content=content, metadata=metadata)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def load_documents(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            documents.extend(simple_text_loader(file_path))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents\n",
      "Created 54 splits\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and process documents\n",
    "# Load documents\n",
    "documents = load_documents('./policies/')\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganjh\\AppData\\Local\\Temp\\ipykernel_19380\\2225740223.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b031103869410a966bbfc038a41b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GJH-Project\\JooHan-SideProject\\lgpg-chatbot\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ganjh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfed3b9db98446ab933909831b6431d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3f5bc5c8a346b6a458ebab2aecff80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1571be0f545d4b24b587af6977900400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee2e19e3f044d7fb98174bf28a2d5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0081a231b4becb69bbcf1bb37445b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4f789dc5c34215a91532a8c01c719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5cd3b82e284d62912217769af9c3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7935a558b9234782aa27b422b5260f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4397811aa2b349d0aaa79061c075c5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb41ac271ac42bfab458276454cb03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create vector store\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"Vector store created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA chain created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganjh\\AppData\\Local\\Temp\\ipykernel_19380\\795319224.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Set up retriever and language model\n",
    "# Initialize retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,  # This should generate about 100 words\n",
    "    temperature=0.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Initialize language model\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"QA chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define question-answering function\n",
    "def ask_question(question):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"], result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hi how are you\n",
      "\n",
      "Answer: I don't know\n",
      "\n",
      "Sources:\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-006.txt\n",
      "- Source: LGPG-GHR-003.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Example usage\n",
    "question2 = \"What is the purpose of the Lion Group Human Resource Manual?\"\n",
    "question = \"Hi how are you\"\n",
    "answer, sources = ask_question(question)\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "print(\"Sources:\")\n",
    "for source in sources:\n",
    "    print(f\"- Source: {source.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235fe725e49344a59b7d7c76b429e540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Hi how are you', description='Question:', placeholder='Type your question here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173dd5467844a3aa8c1e8c98e9af68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask', icon='question', style=ButtonStyle(), tooltip='Click to ask the question')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hi how are you\n",
      "\n",
      "Answer: I don't know\n",
      "\n",
      "Sources:\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-003.txt\n",
      "- Source: LGPG-GHR-006.txt\n",
      "- Source: LGPG-GHR-003.txt\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def on_ask_button_clicked(b):\n",
    "    question = question_widget.value\n",
    "    if question.lower() == 'exit':\n",
    "        print(\"Exiting Q&A session.\")\n",
    "        return\n",
    "    \n",
    "    answer, sources = ask_question(question)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(question_widget)\n",
    "    display(ask_button)\n",
    "    \n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"Sources:\")\n",
    "    for source in sources:\n",
    "        print(f\"- Source: {source.metadata['source']}\")\n",
    "\n",
    "question_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question here',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "ask_button = widgets.Button(\n",
    "    description='Ask',\n",
    "    disabled=False,\n",
    "    button_style='', \n",
    "    tooltip='Click to ask the question',\n",
    "    icon='question'\n",
    ")\n",
    "\n",
    "ask_button.on_click(on_ask_button_clicked)\n",
    "\n",
    "display(question_widget)\n",
    "display(ask_button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
